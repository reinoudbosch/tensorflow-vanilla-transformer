{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e32a853",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "\n",
    "This notebook provides code for building a Dutch to English translator using a vanilla transformer.<br>\n",
    "It uses code from the following sources:\n",
    "\n",
    "https://www.tensorflow.org/text/guide/subwords_tokenizer <br>\n",
    "https://www.tensorflow.org/text/tutorials/transformer <br>\n",
    "https://machinelearningmastery.com/save-load-keras-deep-learning-models/<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c514e6ac",
   "metadata": {},
   "source": [
    "The vanilla transformer was presented in the paper 'Attention is all you need' published by Vashwani et al. (2017) at ArXiv (https://arxiv.org/abs/1706.03762). The architecture is shown in the following image:\n",
    "\n",
    "<img src=\"images/transformer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae69fbe6",
   "metadata": {},
   "source": [
    "The dataset is taken from ParaCrawl (https://paracrawl.eu/) available through tensorflow datasets (https://www.tensorflow.org/datasets/catalog/para_crawl).\n",
    "The model is trained on a single NVIDIA 3090 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439f63d",
   "metadata": {},
   "source": [
    "# Prepare environment for use of GPU and install tensorflow\n",
    "Source: (https://www.tensorflow.org/install/pip)<br><br>\n",
    "<u>Create conda environment:<br></u>\n",
    "conda create --name tf python=3.9 <br>\n",
    "conda activate tf <br><br>\n",
    "<u>check if nvidia gpu driver is installed:<br></u>\n",
    "nvidia-smi <br><br>\n",
    "<u>If needed, install nvidia gpu driver:</u><br>\n",
    "https://www.nvidia.com/Download/index.aspx<br><br>\n",
    "<u>install cuda and cudnn:<br></u>\n",
    "conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0 <br><br>\n",
    "<u>Configure system paths:</u><br>\n",
    "mkdir -p \\\\$CONDA_PREFIX/etc/conda/activate.d <br>\n",
    "echo 'export LD_LIBRARY_PATH=\\\\$LD_LIBRARY_PATH:\\\\$CONDA_PREFIX/lib/' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh <br><br>\n",
    "<u> Install tensorflow:<br></u>\n",
    "pip install --upgrade pip <br>\n",
    "pip install tensorflow <br><br>\n",
    "<u>Some troubleshooting: <br></u>\n",
    "https://stackoverflow.com/questions/44232898/memoryerror-in-tensorflow-and-successful-numa-node-read-from-sysfs-had-negativ <br>\n",
    "https://forums.developer.nvidia.com/t/could-not-load-dynamic-library-libnvinfer-so-7/231606/7<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac0272f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 21:39:58.763185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR) # exclude certain non-essential error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255a8b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU')) # check if GPU is recognized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca1488",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9444515e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='para_crawl',\n",
      "    full_name='para_crawl/ennl/1.2.0',\n",
      "    description=\"\"\"\n",
      "    Web-Scale Parallel Corpora for Official European Languages.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Translation dataset from English to nl.\n",
      "    \"\"\",\n",
      "    homepage='https://paracrawl.eu/releases.html',\n",
      "    data_path='/home/user/tensorflow_datasets/para_crawl/ennl/1.2.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=400.63 MiB,\n",
      "    dataset_size=1.40 GiB,\n",
      "    features=Translation({\n",
      "        'en': Text(shape=(), dtype=string),\n",
      "        'nl': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('en', 'nl'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'train': <SplitInfo num_examples=5659268, num_shards=16>,\n",
      "    },\n",
      "    citation=\"\"\"@misc {paracrawl,\n",
      "        title  = \"ParaCrawl\",\n",
      "        year   = \"2018\",\n",
      "        url    = \"http://paracrawl.eu/download.html.\"\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 21:39:59.855042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 21:40:00.186713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20999 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0a:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('para_crawl/ennl', as_supervised=True, with_info=True)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0e53fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5093341\n"
     ]
    }
   ],
   "source": [
    "# to limit size of dataset use 90% for training, 10% for validation\n",
    "\n",
    "train_examples, train_metadata = tfds.load('para_crawl/ennl', split='train[:90%]',\n",
    "                                     as_supervised=True, with_info=True)\n",
    "print(tf.data.experimental.cardinality(train_examples).numpy()) # elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f90e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565927\n"
     ]
    }
   ],
   "source": [
    "val_examples, val_metadata = tfds.load('para_crawl/ennl', split='train[90%:]',\n",
    "                                     as_supervised=True, with_info=True)\n",
    "print(tf.data.experimental.cardinality(val_examples).numpy()) # elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe9460a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'In the romantically decorated chambre of the Hotel de France the guests are welcomed by a well-stocked fridge, which seems invite you to have a crazy night.', shape=(), dtype=string)\n",
      "tf.Tensor(b'In de romantisch ingerichte chambre van Hotel de France worden de gasten verwelkomd door een gul gevulde ijskast die uit lijkt te nodigen tot een dolle avond.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\"Very well,\" answered my uncle; \"let us start from that point and count four days\\' storm, during which our rate cannot have been less than eighty leagues in the twenty-four hours.\"', shape=(), dtype=string)\n",
      "tf.Tensor(b'\"Goed! Dan zullen wij van dat punt uitgaan en vier dagen storm rekenen, gedurende welke onze snelheid niet minder dan tachtig uur per dag heeft kunnen bedragen.\" \"Dat geloof ik ook.', shape=(), dtype=string)\n",
      "tf.Tensor(b'Price tag 15 nights / 16 days, including top luxury accommodation, air-conditioned car and experienced driver, English-speaking guide, domestic flights in India, breakfast, lunch and dinner every day, ALL monument entrance fees and activities. USD $ 5475 per person', shape=(), dtype=string)\n",
      "tf.Tensor(b'15 nachten / 16 dagen, inclusief Engelssprekende gids, luxe-wagen met airconditioning en chauffeur, verplaatsingen, luxe accommodatie, alle ontbijten, lunches en avondmalen, inkomticketen en de activiteiten die in het overzicht vermeld worden.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# look at first three training examples\n",
    "\n",
    "i = 0\n",
    "for example in train_examples:\n",
    "    print(example[0])\n",
    "    print(example[1])\n",
    "    i += 1\n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46db34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set into Dutch and English\n",
    "train_nl = train_examples.map(lambda en, nl: nl) # for en, nl in train_examples select nl\n",
    "train_en = train_examples.map(lambda en, nl: en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58955199",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23918e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using BERT tokenizer to turn the texts into tokens\n",
    "\n",
    "bert_tokenizer_params=dict(lower_case=True)  # turn text into lowercase\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 32000,   # size of en fr word piece vocabulary used in attention is all you need paper\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Start arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    # https://www.programcreek.com/python/?code=tensorflow%2Ftext%2Ftext-master%2Ftools%2Fwordpiece_vocab%2Fwordpiece_tokenizer_learner_lib.py\n",
    "    # takes in wordcounts (list of (string, int) tuples), returns wordpiece vocabulary\n",
    "    learn_params={},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc82f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db09feb",
   "metadata": {},
   "source": [
    "The bert_vocab_from_dataset algorithm is described here:\n",
    "https://www.tensorflow.org/text/guide/subwords_tokenizer#algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9bac0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !run this cell only once!\n",
    "# # creates Dutch and English vocabularies, takes some time to run (~86 minutes in my case)\n",
    "\n",
    "# nl_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#      train_nl.batch(1000).prefetch(2),\n",
    "#      **bert_vocab_args\n",
    "# )\n",
    "# en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#      train_en.batch(1000).prefetch(2),\n",
    "#      **bert_vocab_args\n",
    "# )\n",
    "\n",
    "\n",
    "# # Check some elements of Dutch vocab\n",
    "# print(nl_vocab[:10])\n",
    "# print(nl_vocab[1000:1100])\n",
    "# print(nl_vocab[-10:])\n",
    "\n",
    "# # write vocabularies to disk, no need to construct them again\n",
    "# def write_vocab_file(filepath, vocab):\n",
    "#   with open(filepath, 'w') as f:\n",
    "#     for token in vocab:\n",
    "#       print(token, file=f)\n",
    "\n",
    "# write_vocab_file('en_vocab.txt', en_vocab)\n",
    "# write_vocab_file('nl_vocab.txt', nl_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24677f0",
   "metadata": {},
   "source": [
    "# Build tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdcce405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54857396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'In de romantisch ingerichte chambre van Hotel de France worden de gasten verwelkomd door een gul gevulde ijskast die uit lijkt te nodigen tot een dolle avond.'\n",
      "b'\"Goed! Dan zullen wij van dat punt uitgaan en vier dagen storm rekenen, gedurende welke onze snelheid niet minder dan tachtig uur per dag heeft kunnen bedragen.\" \"Dat geloof ik ook.'\n",
      "b'15 nachten / 16 dagen, inclusief Engelssprekende gids, luxe-wagen met airconditioning en chauffeur, verplaatsingen, luxe accommodatie, alle ontbijten, lunches en avondmalen, inkomticketen en de activiteiten die in het overzicht vermeld worden.'\n",
      "\n",
      "\n",
      "[701, 696, 7512, 2880, 11234, 2578, 9063, 697, 753, 696, 2956, 718, 696, 1474, 8980, 717, 700, 48, 4688, 12091, 30032, 708, 730, 1567, 705, 7414, 724, 700, 12799, 1146, 2004, 17]\n",
      "[5, 749, 4, 733, 803, 758, 697, 709, 1681, 7518, 698, 1164, 927, 7110, 5053, 15, 1542, 992, 744, 1821, 714, 983, 733, 8886, 935, 830, 820, 732, 746, 6821, 17, 5, 5, 709, 1634, 719, 726, 17]\n",
      "[909, 3887, 18, 1037, 927, 15, 1439, 1736, 23266, 794, 2114, 15, 1494, 16, 3731, 703, 1595, 698, 4953, 15, 16369, 739, 15, 1494, 1225, 15, 743, 15316, 15, 3932, 1149, 698, 2004, 19129, 15, 701, 6852, 7621, 6495, 698, 696, 1170, 708, 701, 699, 916, 2710, 718, 17]\n"
     ]
    }
   ],
   "source": [
    "# explore tokenization with BertTokenizer\n",
    "\n",
    "nl_tokenizer = text.BertTokenizer('nl_vocab.txt', **bert_tokenizer_params)\n",
    "en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)\n",
    "\n",
    "for en_examples, nl_examples in train_examples.batch(3).take(1): # key is (en, nl)\n",
    "    for ex in nl_examples:\n",
    "        print(ex.numpy())\n",
    "    print('\\n')\n",
    "    # Tokenize the examples -> (batch, word, word-piece)\n",
    "    token_batch = nl_tokenizer.tokenize(nl_examples)\n",
    "    # Merge the word and word-piece axes -> (batch, tokens)\n",
    "    token_batch = token_batch.merge_dims(-2,-1)\n",
    "    for ex in token_batch.to_list():\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d02ec247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'in de romantisch ingerichte chambre van hotel de france worden de gasten verwelkomd door een gul gevulde ijskast die uit lijkt te nodigen tot een dolle avond .',\n",
       "       b'\" goed ! dan zullen wij van dat punt uitgaan en vier dagen storm rekenen , gedurende welke onze snelheid niet minder dan tachtig uur per dag heeft kunnen bedragen . \" \" dat geloof ik ook .',\n",
       "       b'15 nachten / 16 dagen , inclusief engelssprekende gids , luxe - wagen met airconditioning en chauffeur , verplaatsingen , luxe accommodatie , alle ontbijten , lunches en avondmalen , inkomticketen en de activiteiten die in het overzicht vermeld worden .'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore how to re-assemble words from extracted tokens\n",
    "\n",
    "words = nl_tokenizer.detokenize(token_batch)\n",
    "tf.strings.reduce_join(words, separator=' ', axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab04ca70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 701, 696, 7512, 2880, 11234, 2578, 9063, 697, 753, 696, 2956, 718, 696,\n",
       "  1474, 8980, 717, 700, 48, 4688, 12091, 30032, 708, 730, 1567, 705, 7414,\n",
       "  724, 700, 12799, 1146, 2004, 17, 3]                                       ,\n",
       " [2, 5, 749, 4, 733, 803, 758, 697, 709, 1681, 7518, 698, 1164, 927, 7110,\n",
       "  5053, 15, 1542, 992, 744, 1821, 714, 983, 733, 8886, 935, 830, 820, 732,\n",
       "  746, 6821, 17, 5, 5, 709, 1634, 719, 726, 17, 3]                        ,\n",
       " [2, 909, 3887, 18, 1037, 927, 15, 1439, 1736, 23266, 794, 2114, 15, 1494,\n",
       "  16, 3731, 703, 1595, 698, 4953, 15, 16369, 739, 15, 1494, 1225, 15, 743,\n",
       "  15316, 15, 3932, 1149, 698, 2004, 19129, 15, 701, 6852, 7621, 6495, 698,\n",
       "  696, 1170, 708, 701, 699, 916, 2710, 718, 17, 3]                        ]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore how to add start and end tokens to examples for tokenization\n",
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "  count = ragged.bounding_shape()[0]\n",
    "  starts = tf.fill([count,1], START)\n",
    "  ends = tf.fill([count,1], END)\n",
    "  return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "add_start_end(token_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "715ec267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a29aae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore how to remove reserved tokens during detokenization\n",
    "\n",
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "  # Drop the reserved tokens, except for \"[UNK]\".\n",
    "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "  bad_token_re = \"|\".join(bad_tokens)\n",
    "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "  # Join them into strings.\n",
    "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dfa6790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'In de romantisch ingerichte chambre van Hotel de France worden de gasten verwelkomd door een gul gevulde ijskast die uit lijkt te nodigen tot een dolle avond.',\n",
       "       b'\"Goed! Dan zullen wij van dat punt uitgaan en vier dagen storm rekenen, gedurende welke onze snelheid niet minder dan tachtig uur per dag heeft kunnen bedragen.\" \"Dat geloof ik ook.',\n",
       "       b'15 nachten / 16 dagen, inclusief Engelssprekende gids, luxe-wagen met airconditioning en chauffeur, verplaatsingen, luxe accommodatie, alle ontbijten, lunches en avondmalen, inkomticketen en de activiteiten die in het overzicht vermeld worden.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_examples.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e6effb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'in', b'de', b'romantisch', b'ingerichte', b'chambre',\n",
       "  b'van', b'hotel', b'de', b'france', b'worden', b'de', b'gasten',\n",
       "  b'verwelkomd', b'door', b'een', b'gul', b'gevulde', b'ijskast', b'die',\n",
       "  b'uit', b'lijkt', b'te', b'nodigen', b'tot', b'een', b'dolle', b'avond',\n",
       "  b'.', b'[END]']                                                         ,\n",
       " [b'[START]', b'\"', b'goed', b'!', b'dan', b'zullen', b'wij', b'van',\n",
       "  b'dat', b'punt', b'uitgaan', b'en', b'vier', b'dagen', b'storm',\n",
       "  b'rekenen', b',', b'gedurende', b'welke', b'onze', b'snelheid', b'niet',\n",
       "  b'minder', b'dan', b'tachtig', b'uur', b'per', b'dag', b'heeft',\n",
       "  b'kunnen', b'bedragen', b'.', b'\"', b'\"', b'dat', b'geloof', b'ik',\n",
       "  b'ook', b'.', b'[END]']                                                 ,\n",
       " [b'[START]', b'15', b'nachten', b'/', b'16', b'dagen', b',', b'inclusief',\n",
       "  b'engelssprekende', b'gids', b',', b'luxe', b'-', b'wagen', b'met',\n",
       "  b'airconditioning', b'en', b'chauffeur', b',', b'verplaatsingen', b',',\n",
       "  b'luxe', b'accommodatie', b',', b'alle', b'ontbijten', b',', b'lunches',\n",
       "  b'en', b'avondmalen', b',', b'inkomticketen', b'en', b'de',\n",
       "  b'activiteiten', b'die', b'in', b'het', b'overzicht', b'vermeld',\n",
       "  b'worden', b'.', b'[END]']                                               ]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_batch = nl_tokenizer.tokenize(nl_examples).merge_dims(-2,-1)\n",
    "token_batch_start_end = add_start_end(token_batch)\n",
    "words = nl_tokenizer.detokenize(token_batch_start_end)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65a68867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'in de romantisch ingerichte chambre van hotel de france worden de gasten verwelkomd door een gul gevulde ijskast die uit lijkt te nodigen tot een dolle avond .',\n",
       "       b'\" goed ! dan zullen wij van dat punt uitgaan en vier dagen storm rekenen , gedurende welke onze snelheid niet minder dan tachtig uur per dag heeft kunnen bedragen . \" \" dat geloof ik ook .',\n",
       "       b'15 nachten / 16 dagen , inclusief engelssprekende gids , luxe - wagen met airconditioning en chauffeur , verplaatsingen , luxe accommodatie , alle ontbijten , lunches en avondmalen , inkomticketen en de activiteiten die in het overzicht vermeld worden .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_text(reserved_tokens, words).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46020272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab660735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now define the customized tokenizer used to tokenize the texts\n",
    "\n",
    "class CustomTokenizer(tf.Module):\n",
    "  def __init__(self, reserved_tokens, vocab_path):\n",
    "    # create tokenizer based on vocab located at vocab_path\n",
    "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "    # [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "    self._reserved_tokens = reserved_tokens\n",
    "    # include vocab file in saved tokenizer model\n",
    "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "    # load vocab\n",
    "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "    # turn vocab into tf.Variable\n",
    "    self.vocab = tf.Variable(vocab)\n",
    "\n",
    "    # signatures serve to identify inputs and outputs of a function when building a model\n",
    "    # these are included in the graph\n",
    "    # get_concrete_function traces the graph, @tf.function runs the graph\n",
    "\n",
    "    ## Create the signatures for export:   \n",
    "\n",
    "    # Include a tokenize signature for a batch of strings. \n",
    "    self.tokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "    # Include `detokenize` and `lookup` signatures for:\n",
    "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "    #   * `RaggedTensors` with shape [batch, tokens]\n",
    "    self.detokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.detokenize.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    self.lookup.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    # These `get_*` methods take no arguments\n",
    "    self.get_vocab_size.get_concrete_function()\n",
    "    self.get_vocab_path.get_concrete_function()\n",
    "    self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "  @tf.function\n",
    "  def tokenize(self, strings):\n",
    "    enc = self.tokenizer.tokenize(strings)\n",
    "    # Merge the `word` and `word-piece` axes.\n",
    "    enc = enc.merge_dims(-2,-1)\n",
    "    enc = add_start_end(enc)\n",
    "    return enc\n",
    "\n",
    "  @tf.function\n",
    "  def detokenize(self, tokenized):\n",
    "    words = self.tokenizer.detokenize(tokenized)\n",
    "    return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "  @tf.function\n",
    "  def lookup(self, token_ids):\n",
    "    return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_size(self):\n",
    "    return tf.shape(self.vocab)[0]\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_path(self):\n",
    "    return self._vocab_path\n",
    "\n",
    "  @tf.function\n",
    "  def get_reserved_tokens(self):\n",
    "    return tf.constant(self._reserved_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a74ee5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build customtokenizers to be used\n",
    "\n",
    "tokenizers = tf.Module()\n",
    "tokenizers.nl = CustomTokenizer(reserved_tokens, 'nl_vocab.txt')\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7494ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save customtokenizers as a tokenizer model\n",
    "\n",
    "tokenizer_model_name = 'tokenizer_model_paracrawl_en_nl_translator'\n",
    "tf.saved_model.save(tokenizers, tokenizer_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b1dac1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detokenize',\n",
       " 'get_reserved_tokens',\n",
       " 'get_vocab_path',\n",
       " 'get_vocab_size',\n",
       " 'lookup',\n",
       " 'name',\n",
       " 'name_scope',\n",
       " 'non_trainable_variables',\n",
       " 'submodules',\n",
       " 'tokenize',\n",
       " 'tokenizer',\n",
       " 'trainable_variables',\n",
       " 'variables',\n",
       " 'vocab',\n",
       " 'with_name_scope']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see methods of tokenizer model module\n",
    "[item for item in dir(tokenizers.en) if not item.startswith('_')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dcbfc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30872"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload model and test methods\n",
    "\n",
    "reloaded_tokenizers = tf.saved_model.load(tokenizer_model_name)\n",
    "reloaded_tokenizers.nl.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ca6a13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    2,  6735,   955, 20622, 25838,     4,     3]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizers.nl.tokenize(['Hallo TensorFlow!'])\n",
    "tokens.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39f9e7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'hallo', b'ten', b'##sor', b'##flow', b'!', b'[END]']]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = reloaded_tokenizers.nl.lookup(tokens)\n",
    "text_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07ffe205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/user/anaconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "updating: tokenizer_model_paracrawl_en_nl_translator/ (stored 0%)\n",
      "updating: tokenizer_model_paracrawl_en_nl_translator/variables/ (stored 0%)\n",
      "updating: tokenizer_model_paracrawl_en_nl_translator/variables/variables.data-00000-of-00001 (deflated 49%)\n",
      "updating: tokenizer_model_paracrawl_en_nl_translator/variables/variables.index (deflated 32%)\n",
      "updating: tokenizer_model_paracrawl_en_nl_translator/assets/ (stored 0%)\n",
      "updating: tokenizer_model_paracrawl_en_nl_translator/assets/nl_vocab.txt (deflated 54%)\n",
      "updating: tokenizer_model_paracrawl_en_nl_translator/assets/en_vocab.txt (deflated 51%)\n",
      "updating: tokenizer_model_paracrawl_en_nl_translator/saved_model.pb (deflated 91%)\n",
      "updating: tokenizer_model_paracrawl_en_nl_translator/fingerprint.pb (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "# archive the tokenizer model\n",
    "!zip -r {tokenizer_model_name}.zip {tokenizer_model_name}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e402210",
   "metadata": {},
   "source": [
    "# Set up data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba1f17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # check the number of tokens per example in the dataset, takes some time (~6 minutes in my case)\n",
    "# # only needs to be run once to determine MAX_TOKENS\n",
    "\n",
    "# lengths = []\n",
    "\n",
    "# for en_examples, nl_examples in train_examples.batch(1024): # key is (en, nl)\n",
    "#     nl_tokens = tokenizers.nl.tokenize(nl_examples)\n",
    "#     lengths.append(nl_tokens.row_lengths())\n",
    "    \n",
    "#     en_tokens = tokenizers.en.tokenize(en_examples)\n",
    "#     lengths.append(en_tokens.row_lengths())\n",
    "\n",
    "# # show the distribution of the number of tokens per example and the maximum\n",
    "\n",
    "# all_lengths = np.concatenate(lengths)\n",
    "\n",
    "# plt.figure(figsize=(14, 6))\n",
    "# plt.hist(all_lengths, np.linspace(0, 400, 101))\n",
    "# plt.xticks(np.arange(0, 400, 10))\n",
    "# plt.ylim(plt.ylim())\n",
    "\n",
    "# max_length = max(all_lengths)\n",
    "# plt.plot([max_length, max_length], plt.ylim())\n",
    "# plt.title(f'Maximum tokens per example: {max_length}');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bf6c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the paper attention is all you need indicates batches were used with around 25,000 tokens\n",
    "# to fit into memory of 1 NVIDIA 3090 GPU here the maximum length of tokens is set to 128 with a batch size of 64\n",
    "\n",
    "MAX_TOKENS=128\n",
    "BATCH_SIZE=64\n",
    "\n",
    "# setting buffer size for shuffling\n",
    "BUFFER_SIZE = 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cfa74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up input pipeline\n",
    "\n",
    "def prepare_batch(en, nl):\n",
    "    nl = tokenizers.nl.tokenize(nl)      # Output is ragged.\n",
    "    nl = nl[:, :MAX_TOKENS]              # Trim to MAX_TOKENS.\n",
    "    nl = nl.to_tensor()                  # Convert to 0-padded dense Tensor\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    en = en[:, :(MAX_TOKENS+1)]\n",
    "    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens\n",
    "\n",
    "    return (nl, en_inputs), en_labels   # nl, en text switched\n",
    "\n",
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2ac0bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 65)\n",
      "(64, 66)\n",
      "(64, 66)\n",
      "tf.Tensor(\n",
      "[[b'[START]' b'veel' b'toeristen' ... b'[PAD]' b'[PAD]' b'[PAD]']\n",
      " [b'[START]' b'laboratorium' b'##apparatuur' ... b'[PAD]' b'[PAD]'\n",
      "  b'[PAD]']\n",
      " [b'[START]' b'u' b'krijgt' ... b'[PAD]' b'[PAD]' b'[PAD]']\n",
      " ...\n",
      " [b'[START]' b'gemiddeld' b'komt' ... b'[PAD]' b'[PAD]' b'[PAD]']\n",
      " [b'[START]' b'virtuele' b'wandeling' ... b'[PAD]' b'[PAD]' b'[PAD]']\n",
      " [b'[START]' b'het' b'derde' ... b'[PAD]' b'[PAD]' b'[PAD]']], shape=(64, 65), dtype=string)\n",
      "tf.Tensor(\n",
      "[[b'[START]' b'many' b'tourists' ... b'[PAD]' b'[PAD]' b'[PAD]']\n",
      " [b'[START]' b'laboratory' b'equipment' ... b'[PAD]' b'[PAD]' b'[PAD]']\n",
      " [b'[START]' b'you' b'also' ... b'[PAD]' b'[PAD]' b'[PAD]']\n",
      " ...\n",
      " [b'[START]' b'on' b'average' ... b'[PAD]' b'[PAD]' b'[PAD]']\n",
      " [b'[START]' b'virtual' b'tour' ... b'[PAD]' b'[PAD]' b'[PAD]']\n",
      " [b'[START]' b'third' b'level' ... b'[PAD]' b'[PAD]' b'[PAD]']], shape=(64, 66), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# test input pipeline\n",
    "\n",
    "for (nl, en_inputs), en_labels in train_batches.take(1):\n",
    "    break\n",
    "    \n",
    "print(nl.shape)\n",
    "print(en_inputs.shape)\n",
    "print(en_labels.shape)\n",
    "print(reloaded_tokenizers.nl.lookup(nl))\n",
    "print(reloaded_tokenizers.en.lookup(en_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb65040",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "This is the left part of the transformer.<br>\n",
    "<img src=\"images/encoder.png\" width=\"300\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c55875",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455700c3",
   "metadata": {},
   "source": [
    "For the input embedding a Keras Embedding layer will be used.<br>\n",
    "\n",
    "The positional encoding is calculated according to the following formula:\n",
    "\n",
    "<img src=\"images/positional_encoding.png\" width=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec802248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1ad4498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_len, d_model):\n",
    "    '''\n",
    "    Returns positional encoding for maximum length and embedding of tokenized sentences\n",
    "    '''\n",
    "    \n",
    "    positions = np.arange(max_len)[:, np.newaxis]                      # shape = (max_len, 1)\n",
    "    i = np.arange(d_model)[np.newaxis, :]//2                           # shape = (1, d_model)\n",
    "    angle_rads = positions / np.power(10000, 2*i/np.float32(d_model))  # shape = (max_len, d_model)\n",
    "    \n",
    "    # apply sin to even indices in the array\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]                          # shape = (batch_size, max_len, d_model)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf865663",
   "metadata": {},
   "source": [
    "The encoder layer will use Keras MultiHeadAttention and LayerNormalization layers.<br>\n",
    "The FeedForward layer can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebb1a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # ouput shape = (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)                  # output shape = (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a35ff0",
   "metadata": {},
   "source": [
    "Now the EncoderLayer can be built, which will be used N times in the encoder.<br>\n",
    "Its MultiHeadAttention layer implements attention which follows this formula:\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\\n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d22b4",
   "metadata": {},
   "source": [
    "The output of the embedding layer has dimensions (batch_size, input_seq_len, d_model). The dimensions of $QK^T$ is (batch_size, input_seq_len, input_seq_len). This means that the padding mask M, used to exclude padding from influencing the softmax calculation, also has dimension (batch_size, input_seq_len, input_seq_len. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "033e9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    '''\n",
    "    Creates attention_mask for MultiHeadAttention layer.\n",
    "    Returns a tensor in which 1 indicates attention and 0 indicates no attention.\n",
    "    Arguments:\n",
    "        seq -- attention logits of dimension (batch_size, seq_len)\n",
    "    Returns:\n",
    "        padding_mask of dimension (batch_size, 1, seq_len)\n",
    "    '''    \n",
    "    \n",
    "    seq = tf.cast(tf.math.not_equal(seq, 0), tf.bool)\n",
    "\n",
    "    return seq[:, tf.newaxis, :]   # shape (batch_size, 1, seq_len). New axis can be broadcast to seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "426f1264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 7), dtype=bool, numpy=\n",
       "array([[[ True,  True,  True,  True,  True, False, False]],\n",
       "\n",
       "       [[ True,  True,  True, False, False, False, False]]])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "create_padding_mask([[32, 64, 87, 5, 368, 0, 0],\n",
    "                     [675, 54, 23, 0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "819e9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63c83f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer consists of a multi-head self-attention layer followed by a feed forward layer.\n",
    "    A residual connection exists around each sub-layer. \n",
    "    This connection is added to the output of the sub-layer, followed by layer normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, h, dff, p_drop=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        '''\n",
    "        d_model -- encoding dimension of the model\n",
    "        h -- number of heads used in MultiHeadAttention layer\n",
    "        dff -- number of nodes in first Dense layer of FeedForward layer\n",
    "        p_drop -- dropout ratio\n",
    "        layernorm_eps -- value added to denominator in calculation of norm, for numerical stability\n",
    "        '''\n",
    "\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=h, key_dim=d_model, dropout=p_drop)\n",
    "\n",
    "        self.ffn = FeedForward(d_model=d_model, dff=dff)\n",
    "\n",
    "        self.layernorm = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(p_drop)\n",
    "    \n",
    "    def call(self, x, training, enc_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- tensor of shape (batch_size, input_seq_len, d_model)\n",
    "            enc_padding_mask -- Boolean mask to ensure padding is not treated as input\n",
    "            training -- Boolean, if set to true activates dropout\n",
    "        Returns:\n",
    "            encoder_layer_out -- tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "                \n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=enc_padding_mask, training=training) \n",
    "        # shape = (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # apply layer normalization on sum of the input and the attention output\n",
    "        out = self.layernorm(Add()([x, attn_output]))\n",
    "\n",
    "        # pass out through a feed forward layer\n",
    "        ffn_output = self.ffn(out)\n",
    "        \n",
    "        # apply dropout layer to ffn output during training\n",
    "        ffn_output =  self.dropout_ffn(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of out1 and ffn_output\n",
    "        encoder_layer_out = self.layernorm(Add()([out, ffn_output]))\n",
    "        \n",
    "        return encoder_layer_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2c9e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ad1874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The Encoder passes the input to an embedding layer, adds positional encoding, \n",
    "    and passes the resulting tensor through a stack of encoder layers.\n",
    "        \n",
    "    \"\"\"  \n",
    "    def __init__(self, input_vocab_size, d_model, max_len, N, h, dff, p_drop=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        '''\n",
    "        Arguments:\n",
    "            input_vocab_size -- size of the vocabulary of the input text, int\n",
    "            d_model -- dimension of the embedding of the model, int\n",
    "            max_len -- maximum length of any input sequence\n",
    "            N -- number of encoding layers, int\n",
    "            h -- number of attention heads per multihead attention layer, int\n",
    "            dff = dimension of fully connected layer\n",
    "            p_drop -- rate of random dropout of elements after each sublayer\n",
    "            layernorm_eps = value added to denominator in calculation of norm, for numerical stability\n",
    "        '''\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, d_model) # creates lookup table with shape (input_vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(max_len, self.d_model)\n",
    "\n",
    "        self.dropout = Dropout(p_drop)\n",
    "\n",
    "        self.N = N\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model=self.d_model, h=h, dff=dff, p_drop=p_drop, layernorm_eps=layernorm_eps)\n",
    "                           for _ in range(self.N)]\n",
    "        \n",
    "    def call(self, x, training, enc_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- tensor of shape (batch_size, input_seq_len)\n",
    "            training -- boolean, if true activates dropout\n",
    "            enc_padding_mask -- boolean mask to ensure that padding is not treated as input\n",
    "        Returns:\n",
    "            x -- tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)          # output shape = (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # Scale embedding by multiplying it with the square root of d_model to enhance performance\n",
    "        x *= tf.sqrt(tf.cast(self.d_model, float))\n",
    "\n",
    "        # Add position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Pass encoded embedding through dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Pass output through stack of encoding layers \n",
    "        for i in range(self.N):\n",
    "            x = self.enc_layers[i](x, training, enc_padding_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3a819",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "This is the right part of the transformer.<br>\n",
    "\n",
    "<img src=\"images/decoder.png\" width=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d89f84",
   "metadata": {},
   "source": [
    "In the decoder, the outputs are shifted right with regard to the inputs. This is done in the data pipeline where \\<SOS> is dropped from the inputs while it is retained in the outputs.<br>\n",
    "After adding positional encoding, in multihead attention a 'causal mask' is added masking out tokens that are still to be predicted. This way, only the tokens that were already predicted in a previous step, or the preceding tokens that are provided during training, become the basis for the relevant query from the decoder to be used on the encoder output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2df2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    mask = tf.linalg.band_part(tf.ones((1, seq_len, seq_len), tf.bool), -1, 0)\n",
    "    return mask  # (1, seq_leng, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca2cb19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 4), dtype=bool, numpy=\n",
       "array([[[ True, False, False, False],\n",
       "        [ True,  True, False, False],\n",
       "        [ True,  True,  True, False],\n",
       "        [ True,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "create_causal_mask(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41a364ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, h, dff, p_drop=0.1, layernorm_eps=1e-6):\n",
    "\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(num_heads=h, key_dim=d_model, dropout=p_drop)\n",
    "        \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "        \n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(p_drop)\n",
    "        \n",
    "    def call(self, x, enc_output, training, causal_mask, dec_padding_mask, enc_padding_mask):\n",
    "\n",
    "        attn1 = self.mha(query=x, value=x, key=x, attention_mask=causal_mask & dec_padding_mask)  \n",
    "        attn1 = self.dropout(attn1, training=training)\n",
    "        out1 = self.layernorm(Add()([x + attn1]))\n",
    "    \n",
    "        attn2 = self.mha(query=out1, value=enc_output, key=enc_output, attention_mask=enc_padding_mask)\n",
    "        attn2 = self.dropout(attn2, training=training)\n",
    "        out2 = self.layernorm(Add()([out1, attn2]))\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout(ffn_output, training=training)\n",
    "        out3 = self.layernorm(Add()([out2, ffn_output]))\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd548a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, max_len, N, h, dff, target_vocab_size, p_drop = 0.1, layernorm_eps = 1e-6):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.N = N\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(max_len, self.d_model)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(p_drop)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model=d_model, h=h, dff=dff, p_drop=p_drop,\n",
    "                                    layernorm_eps = layernorm_eps)\n",
    "                       for _ in range(self.N)]\n",
    "\n",
    "    def call(self, x, enc_output, training, causal_mask, dec_padding_mask, enc_padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)              # output shape = (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        x += self.pos_encoding[:, :seq_len,:]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.N):\n",
    "            x = self.dec_layers[i](x, enc_output, training, causal_mask, dec_padding_mask, enc_padding_mask)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50cdc1",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "<img src=\"images/transformer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ff9d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, input_vocab_size, d_model, max_len, N , h, dff, target_vocab_size, p_drop = 0.1,\n",
    "                 layernorm_eps = 1e-6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_vocab_size = input_vocab_size, d_model = d_model, max_len = max_len,\n",
    "                               N = N, h = h, dff = dff, p_drop = p_drop, layernorm_eps = layernorm_eps)\n",
    "    \n",
    "        self.decoder = Decoder(d_model = d_model, max_len = max_len, N = N, h = h, dff = dff,\n",
    "                               target_vocab_size = target_vocab_size, p_drop = p_drop, layernorm_eps = layernorm_eps)\n",
    "    \n",
    "        self.linear_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "\n",
    "        enc_inputs, dec_inputs = inputs\n",
    "        \n",
    "        enc_padding_mask, causal_mask, dec_padding_mask = self.create_masks(enc_inputs, dec_inputs)\n",
    "    \n",
    "        enc_output = self.encoder(enc_inputs, training, enc_padding_mask)\n",
    "    \n",
    "        dec_output = self.decoder(dec_inputs, enc_output, training, causal_mask, dec_padding_mask, enc_padding_mask)\n",
    "\n",
    "        logits = self.linear_layer(dec_output)\n",
    "    \n",
    "        # return output of linear layer of decoder (before softmax)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def create_masks(self, enc_inputs, dec_inputs):\n",
    "        \n",
    "        enc_padding_mask = create_padding_mask(enc_inputs)\n",
    "        \n",
    "        causal_mask = create_causal_mask(tf.shape(dec_inputs)[1])\n",
    "\n",
    "        dec_padding_mask = create_padding_mask(dec_inputs)\n",
    "            \n",
    "        return enc_padding_mask, causal_mask, dec_padding_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cc10b",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4dc9b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6                 # number of layers\n",
    "d_model = 512         # embedding dimension of model\n",
    "max_len = MAX_TOKENS  # maximum length of any input sequence\n",
    "dff = 2048            # number of nodes of feedforward layer\n",
    "h = 8                 # number of heads\n",
    "p_drop = 0.1          # dropout rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "588f083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    input_vocab_size=tokenizers.nl.get_vocab_size().numpy(),\n",
    "    d_model = d_model,\n",
    "    max_len = max_len,\n",
    "    N = N,\n",
    "    h = h,\n",
    "    dff=dff,\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    p_drop=p_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3d68689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 21:40:05.554229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "output = transformer((nl, en_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38965a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 65)\n",
      "(64, 66)\n",
      "(64, 66, 31222)\n"
     ]
    }
   ],
   "source": [
    "print(nl.shape)\n",
    "print(en_inputs.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "559a7a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  78819328  \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  78998528  \n",
      "                                                                 \n",
      " dense_24 (Dense)            multiple                  16016886  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,834,742\n",
      "Trainable params: 173,834,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb353a",
   "metadata": {},
   "source": [
    "# Set learning rate schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fd252",
   "metadata": {},
   "source": [
    "The following learning rate schedule is used to train the model:\n",
    "<img src=\"images/learning_rate.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6d0ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da296de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueError: Unknown layer: 'Transformer'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f131151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10833a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537391c0",
   "metadata": {},
   "source": [
    "# Set up loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dde39ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea7ac69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss = loss_object(label, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39fdd65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4aa8c0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 21:40:17.005128: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1d38fce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-12 21:40:17.005151: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-01-12 21:40:17.009200: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-01-12 21:40:17.086826: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79584/79584 [==============================] - 37184s 467ms/step - loss: 4.3922 - masked_accuracy: 0.3419 - val_loss: 6.3373 - val_masked_accuracy: 0.1311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7db0deda00>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 epoch takes around 11 hours on 1 NVIDIA GeForce RTX 3090 GPU. Steps taken 79,584\n",
    "\n",
    "transformer.fit(train_batches,\n",
    "                epochs=1,\n",
    "                validation_data = val_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc201674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "\n",
    "    def __init__(self, tokenizers, transformer):\n",
    "        self.tokenizers = tokenizers\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "        # The input sentence is Dutch, hence adding the `[START]` and `[END]` tokens.\n",
    "        assert isinstance(sentence, tf.Tensor)\n",
    "        if len(sentence.shape) == 0:\n",
    "          sentence = sentence[tf.newaxis]\n",
    "\n",
    "        sentence = self.tokenizers.nl.tokenize(sentence).to_tensor()\n",
    "\n",
    "        encoder_input = sentence\n",
    "\n",
    "        # As the output language is English, initialize the output with the\n",
    "        # English `[START]` token.\n",
    "        start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "        start = start_end[0][tf.newaxis]\n",
    "        end = start_end[1][tf.newaxis]\n",
    "\n",
    "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "          output = tf.transpose(output_array.stack())\n",
    "          predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "          # Select the last token from the `seq_len` dimension.\n",
    "          predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "          predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "          # Concatenate the `predicted_id` to the output which is given to the\n",
    "          # decoder as its input.\n",
    "          output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "          if predicted_id == end:\n",
    "            break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # The output shape is `(1, tokens)`.\n",
    "        text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
    "\n",
    "        tokens = tokenizers.en.lookup(output)[0]\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop.\n",
    "        # So, recalculate them outside the loop.\n",
    "        self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "\n",
    "        return text, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd197e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80d81429",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'eens kijken of dit goed vertaald wordt.'\n",
    "ground_truth = 'let\\'s see if this is translated correctly.'\n",
    "\n",
    "translated_text, translated_tokens = translator(\n",
    "    tf.constant(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0df8495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : eens kijken of dit goed vertaald wordt.\n",
      "Prediction     : the following is the result :\n",
      "Ground truth   : let's see if this is translated correctly.\n"
     ]
    }
   ],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')\n",
    "\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9beca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is not great. Ways to increase accuracy could include: \n",
    "# check for bugs, train multiple epochs, improve the dataset, adjust hyperparameters, add layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e31c4eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "transformer_json = transformer.to_json()\n",
    "with open(\"transformer.json\", \"w\") as json_file:\n",
    "    json_file.write(transformer_json)\n",
    "# serialize weights to HDF5\n",
    "transformer.save_weights(\"transformer.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66534ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
